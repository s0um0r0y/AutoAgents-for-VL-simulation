{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUk3Y0WwYZ4"
      },
      "source": [
        "# ğŸ¤— x ğŸ¦¾: Training SmolVLA with LeRobot Notebook\n",
        "\n",
        "Welcome to the **LeRobot SmolVLA training notebook**! This notebook provides a ready-to-run setup for training imitation learning policies using the [ğŸ¤— LeRobot](https://github.com/huggingface/lerobot) library.\n",
        "\n",
        "In this example, we train an `SmolVLA` policy using a dataset hosted on the [Hugging Face Hub](https://huggingface.co/), and optionally track training metrics with [Weights & Biases (wandb)](https://wandb.ai/).\n",
        "\n",
        "## âš™ï¸ Requirements\n",
        "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=YOUR_USERNAME/YOUR_DATASET`)\n",
        "- Optional: A [wandb](https://wandb.ai/) account if you want to enable training visualization\n",
        "- Recommended: GPU runtime (e.g., NVIDIA A100) for faster training\n",
        "\n",
        "## â±ï¸ Expected Training Time\n",
        "Training with the `SmolVLA` policy for 20,000 steps typically takes **about 5 hours on an NVIDIA A100** GPU. On less powerful GPUs or CPUs, training may take significantly longer!\n",
        "\n",
        "## Example Output\n",
        "Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`. If `wandb` is enabled, progress will also be visualized in your wandb project dashboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOJyX0CnwA5m"
      },
      "source": [
        "## Install conda\n",
        "This cell uses `condacolab` to bootstrap a full Conda environment inside Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlKjL1X5t_zM",
        "outputId": "6f3ef11e-68ca-4518-de71-68a618ceb68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:09\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxCc3CARwUjN"
      },
      "source": [
        "## Install LeRobot\n",
        "This cell clones the `lerobot` repository from Hugging Face, installs FFmpeg (version 7.1.1), and installs the package in editable mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgLu7QT5tUik"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge\n",
        "!cd lerobot && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Sn2wG4wldo"
      },
      "source": [
        "## Weights & Biases login\n",
        "This cell logs you into Weights & Biases (wandb) to enable experiment tracking and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PolVM_movEvp"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2qKVhteja8Q"
      },
      "source": [
        "## Install SmolVLA dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SrBO8qhfja8Q",
        "outputId": "e919940e-fabf-42ad-df87-4a2f2efd1cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets>=4.0.0 (from lerobot==0.3.4)\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting diffusers>=0.27.2 (from lerobot==0.3.4)\n",
            "  Downloading diffusers-0.35.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting huggingface-hub>=0.34.2 (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting cmake>=3.29.0.1 (from lerobot==0.3.4)\n",
            "  Downloading cmake-4.1.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting einops>=0.8.0 (from lerobot==0.3.4)\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting opencv-python-headless>=4.9.0 (from lerobot==0.3.4)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting av>=14.2.0 (from lerobot==0.3.4)\n",
            "  Downloading av-15.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=4.0.0 (from lerobot==0.3.4)\n",
            "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.3.4) (24.2)\n",
            "Collecting pynput>=1.7.7 (from lerobot==0.3.4)\n",
            "  Using cached pynput-1.8.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting pyserial>=3.5 (from lerobot==0.3.4)\n",
            "  Using cached pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting wandb>=0.20.0 (from lerobot==0.3.4)\n",
            "  Downloading wandb-0.22.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting torch<2.8.0,>=2.2.1 (from lerobot==0.3.4)\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchcodec<0.6.0,>=0.2.1 (from lerobot==0.3.4)\n",
            "  Downloading torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting torchvision<0.23.0,>=0.21.0 (from lerobot==0.3.4)\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting draccus==0.10.0 (from lerobot==0.3.4)\n",
            "  Using cached draccus-0.10.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting gymnasium<1.0.0,>=0.29.1 (from lerobot==0.3.4)\n",
            "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rerun-sdk<0.23.0,>=0.21.0 (from lerobot==0.3.4)\n",
            "  Using cached rerun_sdk-0.22.1-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting deepdiff<9.0.0,>=7.0.1 (from lerobot==0.3.4)\n",
            "  Using cached deepdiff-8.6.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting imageio<3.0.0,>=2.34.0 (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.3.4)\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting termcolor<4.0.0,>=2.4.0 (from lerobot==0.3.4)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting mergedeep~=1.3 (from draccus==0.10.0->lerobot==0.3.4)\n",
            "  Using cached mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pyyaml~=6.0 (from draccus==0.10.0->lerobot==0.3.4)\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pyyaml-include~=1.4 (from draccus==0.10.0->lerobot==0.3.4)\n",
            "  Using cached pyyaml_include-1.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting toml~=0.10 (from draccus==0.10.0->lerobot==0.3.4)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect~=0.9.0 (from draccus==0.10.0->lerobot==0.3.4)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting num2words>=0.5.14 (from lerobot==0.3.4)\n",
            "  Using cached num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate>=1.7.0 (from lerobot==0.3.4)\n",
            "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting safetensors>=0.4.3 (from lerobot==0.3.4)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting numpy<3.0.0,>=1.17 (from accelerate>=1.7.0->lerobot==0.3.4)\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting psutil (from accelerate>=1.7.0->lerobot==0.3.4)\n",
            "  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting filelock (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets>=4.0.0->lerobot==0.3.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets>=4.0.0->lerobot==0.3.4) (4.67.1)\n",
            "Collecting xxhash (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting orderly-set<6,>=5.4.1 (from deepdiff<9.0.0,>=7.0.1->lerobot==0.3.4)\n",
            "  Using cached orderly_set-5.5.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting importlib_metadata (from diffusers>=0.27.2->lerobot==0.3.4)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting regex!=2019.12.17 (from diffusers>=0.27.2->lerobot==0.3.4)\n",
            "  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting Pillow (from diffusers>=0.27.2->lerobot==0.3.4)\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4)\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions>=4.3.0 (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.0.0,>=0.29.1->lerobot==0.3.4)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.34.2->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Using cached InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Using cached pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting prompt-toolkit<4.0.0,>=3.0.1 (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting imageio-ffmpeg (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.3.4)\n",
            "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting attrs>=19.2.0 (from jsonlines>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.14->lerobot==0.3.4)\n",
            "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy<3.0.0,>=1.17 (from accelerate>=1.7.0->lerobot==0.3.4)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting six (from pynput>=1.7.7->lerobot==0.3.4)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting evdev>=1.3 (from pynput>=1.7.7->lerobot==0.3.4)\n",
            "  Using cached evdev-1.9.2.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-xlib>=0.17 (from pynput>=1.7.7->lerobot==0.3.4)\n",
            "  Using cached python_xlib-0.33-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting sympy>=1.13.3 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.1->torch<2.8.0,>=2.2.1->lerobot==0.3.4) (65.6.3)\n",
            "Collecting click>=8.0.1 (from wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb>=0.20.0->lerobot==0.3.4) (4.3.6)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pydantic<3 (from wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading sentry_sdk-2.39.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting transformers>=4.52.0 (from lerobot==0.3.4)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=4.0.0->lerobot==0.3.4) (2024.12.14)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.52.0->lerobot==0.3.4)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.3.4)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting zipp>=3.20 (from importlib_metadata->diffusers>=0.27.2->lerobot==0.3.4)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.3.4)\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=4.0.0->lerobot==0.3.4)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.20.0->lerobot==0.3.4)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.34.2->lerobot==0.3.4)\n",
            "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Using cached draccus-0.10.0-py3-none-any.whl (71 kB)\n",
            "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
            "Downloading av-15.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.6/39.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmake-4.1.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (29.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "Using cached deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
            "Downloading diffusers-0.35.1-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Using cached num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pynput-1.8.1-py2.py3-none-any.whl (91 kB)\n",
            "Using cached pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "Using cached rerun_sdk-0.22.1-cp38-abi3-manylinux_2_31_x86_64.whl (51.4 MB)\n",
            "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.22.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached orderly_set-5.5.0-py3-none-any.whl (13 kB)\n",
            "Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
            "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached python_xlib-0.33-py2.py3-none-any.whl (182 kB)\n",
            "Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pyyaml_include-1.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.39.0-py2.py3-none-any.whl (370 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m160.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
            "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: lerobot, docopt, evdev\n",
            "  Building editable for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.3.4-0.editable-py3-none-any.whl size=15290 sha256=d977e2ab6897c50179597519a8c5e1aa6e3c203873f98736dead9a265b6dd304\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-57ipulep/wheels/15/0d/02/b9c6ff1c78574dee99101ad231194b3425eb4cd784ce8c8338\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9e756b32207a5615de9d3d91180643fbde0b616c1cf2721892e459db9d3872e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for evdev (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evdev: filename=evdev-1.9.2-cp311-cp311-linux_x86_64.whl size=74951 sha256=a175adee260dd04af717532a3b70f7b4e32652cb39286829c841a369dd445e36\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/97/d0/ea1b02915719d1cdb6a8810aa7683524c7aceedc5812cdeed7\n",
            "Successfully built lerobot docopt evdev\n",
            "Installing collected packages: pytz, pyserial, nvidia-cusparselt-cu12, mpmath, farama-notifications, docopt, zipp, xxhash, wcwidth, tzdata, typing-extensions, triton, torchcodec, toml, termcolor, sympy, smmap, six, sentry-sdk, safetensors, regex, pyyaml, pyarrow, psutil, protobuf, propcache, Pillow, pfzy, orderly-set, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, num2words, networkx, mypy-extensions, multidict, mergedeep, MarkupSafe, imageio-ffmpeg, hf-xet, hf-transfer, fsspec, frozenlist, filelock, evdev, einops, dill, cmake, cloudpickle, click, av, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, rerun-sdk, pyyaml-include, python-xlib, python-dateutil, pydantic-core, prompt-toolkit, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jsonlines, jinja2, importlib_metadata, imageio, huggingface-hub, gymnasium, gitdb, deepdiff, aiosignal, tokenizers, pynput, pydantic, pandas, nvidia-cusolver-cu12, InquirerPy, gitpython, draccus, diffusers, aiohttp, wandb, transformers, torch, torchvision, datasets, accelerate, lerobot\n",
            "Successfully installed InquirerPy-0.3.4 MarkupSafe-3.0.3 Pillow-11.3.0 accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 attrs-25.3.0 av-15.1.0 click-8.3.0 cloudpickle-3.1.1 cmake-4.1.0 datasets-4.1.1 deepdiff-8.6.1 diffusers-0.35.1 dill-0.4.0 docopt-0.6.2 draccus-0.10.0 einops-0.8.1 evdev-1.9.2 farama-notifications-0.0.4 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.9.0 gitdb-4.0.12 gitpython-3.1.45 gymnasium-0.29.1 hf-transfer-0.1.9 hf-xet-1.1.10 huggingface-hub-0.35.1 imageio-2.37.0 imageio-ffmpeg-0.6.0 importlib_metadata-8.7.0 jinja2-3.1.6 jsonlines-4.0.0 lerobot-0.3.4 mergedeep-1.3.4 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 mypy-extensions-1.1.0 networkx-3.5 num2words-0.5.14 numpy-2.2.6 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-headless-4.12.0.88 orderly-set-5.5.0 pandas-2.3.2 pfzy-0.3.4 prompt-toolkit-3.0.52 propcache-0.3.2 protobuf-6.32.1 psutil-7.1.0 pyarrow-21.0.0 pydantic-2.11.9 pydantic-core-2.33.2 pynput-1.8.1 pyserial-3.5 python-dateutil-2.9.0.post0 python-xlib-0.33 pytz-2025.2 pyyaml-6.0.3 pyyaml-include-1.4.1 regex-2025.9.18 rerun-sdk-0.22.1 safetensors-0.6.2 sentry-sdk-2.39.0 six-1.17.0 smmap-5.0.2 sympy-1.14.0 termcolor-3.1.0 tokenizers-0.22.1 toml-0.10.2 torch-2.7.1 torchcodec-0.5 torchvision-0.22.1 transformers-4.56.2 triton-3.3.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.1 tzdata-2025.2 wandb-0.22.0 wcwidth-0.2.14 xxhash-3.5.0 yarl-1.20.1 zipp-3.23.0\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && pip install -e \".[smolvla]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkzTo4mNwxaC"
      },
      "source": [
        "## Start training SmolVLA with LeRobot\n",
        "\n",
        "This cell runs the `train.py` script from the `lerobot` library to train a robot control policy.  \n",
        "\n",
        "Make sure to adjust the following arguments to your setup:\n",
        "\n",
        "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
        "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `pepijn223/il_gym0`.\n",
        "\n",
        "2. `--batch_size=64`: means the model processes 64 training samples in parallel before doing one gradient update. Reduce this number if you have a GPU with low memory.\n",
        "\n",
        "3. `--output_dir=outputs/train/...`:  \n",
        "   Directory where training logs and model checkpoints will be saved.\n",
        "\n",
        "4. `--job_name=...`:  \n",
        "   A name for this training job, used for logging and Weights & Biases.\n",
        "\n",
        "5. `--policy.device=cuda`:  \n",
        "   Use `cuda` if training on an NVIDIA GPU. Use `mps` for Apple Silicon, or `cpu` if no GPU is available.\n",
        "\n",
        "6. `--wandb.enable=true`:  \n",
        "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-2Bz7Xyja8R",
        "outputId": "134de582-6f10-488f-cd2a-38b5a6766f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 2025-09-28 13:45:01 ils/utils.py:41 Cuda backend detected, using cuda.\n",
            "WARNING 2025-09-28 13:45:01 /policies.py:79 Device 'None' is not available. Switching to 'cuda'.\n",
            "INFO 2025-09-28 13:45:01 ot_train.py:148 {'batch_size': 64,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 's0um0r0y/record-test',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': 'outputs/train/my_smolvla',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.image': {'shape': [3, 256, 256],\n",
            "                                                     'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.image2': {'shape': [3, 256, 256],\n",
            "                                                      'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.image3': {'shape': [3, 256, 256],\n",
            "                                                      'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 's0um0r0y/my_smolvla_model',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 20000,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': True,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "\u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
            "INFO 2025-09-28 13:45:03 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/eshaan-rithesh2023-vit-chennai/lerobot/runs/pknkn76f\u001b[0m\n",
            "INFO 2025-09-28 13:45:03 ot_train.py:164 Creating dataset\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "meta/episodes/chunk-000/file-000.parquet:   0% 0.00/69.2k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "meta/tasks.parquet:   0% 0.00/2.29k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "info.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "info.json: 2.60kB [00:00, 658kB/s]\n",
            "stats.json: 4.53kB [00:00, 4.08MB/s]\n",
            "\n",
            "\n",
            "meta/tasks.parquet: 100% 2.29k/2.29k [00:02<00:00, 1.00kB/s]\n",
            "\n",
            "meta/episodes/chunk-000/file-000.parquet: 100% 69.2k/69.2k [00:02<00:00, 29.4kB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:02<00:00,  1.35it/s]\n",
            "Generating train split: 52 examples [00:00, 2769.57 examples/s]\n",
            "Fetching 8 files:   0% 0/8 [00:00<?, ?it/s]\n",
            "videos/observation.images.front/chunk-00(â€¦):   0% 0.00/69.0M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/file-000.parquet:   0% 0.00/604k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            ".gitattributes: 2.46kB [00:00, 3.45MB/s]\n",
            "Fetching 8 files:  12% 1/8 [00:00<00:05,  1.31it/s]\n",
            "\n",
            "\n",
            "README.md: 3.08kB [00:00, 9.22MB/s]\n",
            "\n",
            "\n",
            "data/chunk-000/file-000.parquet: 100% 604k/604k [00:00<00:00, 1.17MB/s]\n",
            "Fetching 8 files:  38% 3/8 [00:01<00:01,  3.23it/s]\n",
            "videos/observation.images.front/chunk-00(â€¦):   3% 1.91M/69.0M [00:02<01:25, 780kB/s]\u001b[A\n",
            "videos/observation.images.front/chunk-00(â€¦): 100% 69.0M/69.0M [00:06<00:00, 10.3MB/s]\n",
            "Fetching 8 files: 100% 8/8 [00:07<00:00,  1.10it/s]\n",
            "Generating train split: 19579 examples [00:00, 315972.40 examples/s]\n",
            "INFO 2025-09-28 13:45:15 ot_train.py:175 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "config.json: 3.77kB [00:00, 5.38MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 2.03G/2.03G [00:20<00:00, 101MB/s]\n",
            "INFO 2025-09-28 13:45:37 odeling.py:1004 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "generation_config.json: 100% 136/136 [00:00<00:00, 532kB/s]\n",
            "processor_config.json: 100% 67.0/67.0 [00:00<00:00, 319kB/s]\n",
            "chat_template.json: 100% 430/430 [00:00<00:00, 2.09MB/s]\n",
            "preprocessor_config.json: 100% 599/599 [00:00<00:00, 2.58MB/s]\n",
            "tokenizer_config.json: 28.6kB [00:00, 31.8MB/s]\n",
            "vocab.json: 801kB [00:00, 9.60MB/s]\n",
            "merges.txt: 466kB [00:00, 9.43MB/s]\n",
            "tokenizer.json: 3.55MB [00:00, 30.0MB/s]\n",
            "added_tokens.json: 4.74kB [00:00, 11.1MB/s]\n",
            "special_tokens_map.json: 100% 868/868 [00:00<00:00, 4.11MB/s]\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "model.safetensors: 100% 907M/907M [00:16<00:00, 55.8MB/s]\n",
            "WARNING 2025-09-28 13:46:15 ies/utils.py:85 Missing key(s) when loading model: {'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.state_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight', 'model.action_out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.action_out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight', 'model.state_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.embed_tokens.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.norm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.norm.weight', 'model.action_in_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias', 'model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight', 'model.action_time_mlp_out.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.action_in_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.lm_head.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight', 'model.action_time_mlp_out.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight', 'model.action_time_mlp_in.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.action_time_mlp_in.bias', 'model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight', 'model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight', 'model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight', 'model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight', 'model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight', 'model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight', 'model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight'}\n",
            "WARNING 2025-09-28 13:46:16 ies/utils.py:87 Unexpected key(s) when loading model: {'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'unnormalize_outputs.so100_buffer_action.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight', 'unnormalize_outputs.so100-blue_buffer_action.std', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight', 'unnormalize_outputs.so100-red_buffer_action.std', 'normalize_inputs.so100_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias', 'unnormalize_outputs.so100-blue_buffer_action.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight', 'model._orig_mod.action_in_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight', 'normalize_inputs.so100-blue_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias', 'normalize_inputs.so100-blue_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight', 'normalize_inputs.so100_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model._orig_mod.action_time_mlp_in.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight', 'normalize_targets.so100-red_buffer_action.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model._orig_mod.action_in_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.norm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight', 'model._orig_mod.action_out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight', 'normalize_inputs.so100-red_buffer_observation_state.mean', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.norm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight', 'model._orig_mod.state_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight', 'model._orig_mod.state_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight', 'model._orig_mod.action_time_mlp_out.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight', 'unnormalize_outputs.so100_buffer_action.std', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias', 'normalize_targets.so100-blue_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'unnormalize_outputs.so100-red_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight', 'model._orig_mod.action_out_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.embed_tokens.weight', 'model._orig_mod.vlm_with_expert.vlm.lm_head.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight', 'normalize_targets.so100-blue_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight', 'normalize_inputs.so100-red_buffer_observation_state.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight', 'model._orig_mod.action_time_mlp_out.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight', 'normalize_targets.so100_buffer_action.mean', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias', 'model._orig_mod.action_time_mlp_in.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight', 'normalize_targets.so100_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'normalize_targets.so100-red_buffer_action.std', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight', 'model._orig_mod.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model._orig_mod.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias'}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 407, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/lerobot/smolvla_base/resolve/main/policy_preprocessor.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lerobot/src/lerobot/processor/pipeline.py\", line 654, in _load_config\n",
            "    config_path = hf_hub_download(\n",
            "                  ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1073, in _hf_hub_download_to_cache_dir\n",
            "    (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = _get_metadata_or_catch_error(\n",
            "                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 418, in hf_raise_for_status\n",
            "    raise _format(EntryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-68d93c28-000b82a93df7df7e529570b2;ade28f2b-395a-496a-98bb-635137718dd4)\n",
            "\n",
            "Entry Not Found for url: https://huggingface.co/lerobot/smolvla_base/resolve/main/policy_preprocessor.json.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lerobot-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/scripts/lerobot_train.py\", line 359, in main\n",
            "    train()\n",
            "  File \"/content/lerobot/src/lerobot/configs/parser.py\", line 225, in wrapper_inner\n",
            "    response = fn(cfg, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/scripts/lerobot_train.py\", line 196, in train\n",
            "    preprocessor, postprocessor = make_pre_post_processors(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/policies/factory.py\", line 203, in make_pre_post_processors\n",
            "    PolicyProcessorPipeline.from_pretrained(\n",
            "  File \"/content/lerobot/src/lerobot/processor/pipeline.py\", line 567, in from_pretrained\n",
            "    loaded_config, base_path = cls._load_config(model_id, config_filename, hub_download_kwargs)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lerobot/src/lerobot/processor/pipeline.py\", line 665, in _load_config\n",
            "    raise FileNotFoundError(\n",
            "FileNotFoundError: Could not find 'policy_preprocessor.json' on the HuggingFace Hub at 'lerobot/smolvla_base'\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && lerobot-train \\\n",
        "  --policy.path=cijerezg/smolvla-test \\\n",
        "  --policy.repo_id=s0um0r0y/my_smolvla_model \\\n",
        "  --dataset.repo_id=s0um0r0y/record-test \\\n",
        "  --batch_size=64 \\\n",
        "  --steps=20000 \\\n",
        "  --output_dir=outputs/train/my_smolvla \\\n",
        "  --job_name=my_smolvla_training \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr0VwiFRja8R"
      },
      "source": [
        "## Login into Hugging Face Hub\n",
        "Now after training is done login into the Hugging Face hub and upload the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yu5khQGIHi6"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFMLGuVkH7UN"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli upload ${HF_USER}/my_smolvla \\\n",
        "  /content/lerobot/outputs/train/my_smolvla/checkpoints/last/pretrained_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}