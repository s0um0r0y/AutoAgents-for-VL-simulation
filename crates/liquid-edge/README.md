# Liquid Edge - High-Performance Edge Inference Runtime

Liquid Edge is a production-ready inference runtime designed specifically for edge computing environments. It provides high-performance LLM inference with multiple backend support, comprehensive tokenization capabilities, and optimized memory management.

## ğŸš€ Features

- **ğŸ¯ Multiple Backends**: ONNX Runtime support with more backends planned
- **âš¡ High Performance**: Optimized for edge devices with minimal latency
- **ğŸ”„ Async Support**: Full async/await support for non-blocking inference
- **ğŸ’¬ Chat Templates**: Jinja2 template support for conversational AI
- **ğŸ“Š Comprehensive Monitoring**: Built-in metrics and performance tracking
- **ğŸ›¡ï¸ Production Ready**: Robust error handling and logging
- **ğŸ”§ Flexible Configuration**: Feature flags for customized builds


```sh
pipx install "optimum[onnxruntime]"
optimum-cli export onnx \                                                                                                                                              î‚² âœ” â”‚ 7s ï‰’  â”‚ rust-onnx-chat îœ¼
      --model squeeze-ai-lab/TinyAgent-1.1B \
      --task text-generation ./models/tinyagen
```

### Model Directory Structure

```
models/my-model/
â”œâ”€â”€ model.onnx              # ONNX model file
â”œâ”€â”€ tokenizer.json          # HuggingFace tokenizer
â”œâ”€â”€ config.json             # Model configuration
â”œâ”€â”€ tokenizer_config.json   # Tokenizer configuration
â”œâ”€â”€ special_tokens_map.json # Special tokens mapping
â””â”€â”€ chat_template.jinja     # Chat template (optional)
```
